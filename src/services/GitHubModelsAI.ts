/**
 * ü§ñ GITHUB MODELS AI INTEGRATION
 * 
 * GitHub Models oferece acesso gratuito a:
 * - GPT-4o, GPT-4o mini
 * - Claude 3.5 Sonnet  
 * - Llama 3.1 405B
 * - Phi-3.5 MoE, Phi-3.5 Vision
 * - Mistral Large 2, Mistral Nemo
 * - Cohere Command R, Command R+
 * 
 * Limite: 15 RPM (requests per minute) - Perfeito para desenvolvimento
 */

export interface GitHubModelsConfig {
    token: string; // GitHub Personal Access Token
    model: 'gpt-4o' | 'gpt-4o-mini' | 'claude-3.5-sonnet' | 'llama-3.1-405b' | 'phi-3.5-moe' | 'mistral-large-2';
    maxTokens?: number;
    temperature?: number;
}

export interface AIRequest {
    messages: Array<{
        role: 'system' | 'user' | 'assistant';
        content: string;
    }>;
    maxTokens?: number;
    temperature?: number;
}

export interface AIResponse {
    content: string;
    usage: {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
    };
    model: string;
}

export class GitHubModelsAI {
    private config: GitHubModelsConfig;
    private baseUrl = 'https://models.inference.ai.azure.com';

    constructor(config: GitHubModelsConfig) {
        this.config = config;
    }

    /**
     * üéØ Gerar conte√∫do usando GitHub Models
     */
    async generateContent(request: AIRequest): Promise<AIResponse> {
        try {
            const response = await fetch(`${this.baseUrl}/chat/completions`, {
                method: 'POST',
                headers: {
                    'Content-Type': 'application/json',
                    'Authorization': `Bearer ${this.config.token}`,
                },
                body: JSON.stringify({
                    messages: request.messages,
                    model: this.config.model,
                    max_tokens: request.maxTokens || this.config.maxTokens || 1000,
                    temperature: request.temperature || this.config.temperature || 0.7,
                }),
            });

            if (!response.ok) {
                throw new Error(`GitHub Models API error: ${response.statusText}`);
            }

            const data = await response.json();

            return {
                content: data.choices[0].message.content,
                usage: {
                    promptTokens: data.usage.prompt_tokens,
                    completionTokens: data.usage.completion_tokens,
                    totalTokens: data.usage.total_tokens,
                },
                model: this.config.model,
            };
        } catch (error) {
            console.error('‚ùå GitHub Models API Error:', error);
            throw error;
        }
    }

    /**
     * üé® Gerar templates de quiz usando IA
     */
    async generateQuizTemplate(prompt: string): Promise<any> {
        const request: AIRequest = {
            messages: [
                {
                    role: 'system',
                    content: `Voc√™ √© um especialista em cria√ß√£o de quizzes interativos. 
          Gere um quiz estruturado com base no prompt do usu√°rio.
          Retorne apenas JSON v√°lido com esta estrutura:
          {
            "title": "string",
            "description": "string",
            "steps": [
              {
                "question": "string",
                "type": "multiple-choice | single-choice | text-input",
                "options": ["option1", "option2", "option3", "option4"],
                "correctAnswer": "string (opcional)"
              }
            ]
          }`
                },
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: 0.8,
            maxTokens: 2000
        };

        const response = await this.generateContent(request);

        try {
            return JSON.parse(response.content);
        } catch (error) {
            console.error('‚ùå Erro ao parsear JSON da IA:', error);
            throw new Error('IA retornou formato inv√°lido');
        }
    }

    /**
     * üöÄ Gerar steps de funil usando IA
     */
    async generateFunnelSteps(prompt: string): Promise<any[]> {
        const request: AIRequest = {
            messages: [
                {
                    role: 'system',
                    content: `Voc√™ √© um especialista em marketing digital e funis de convers√£o.
          Crie steps de funil baseados no prompt do usu√°rio.
          Retorne apenas um JSON array com esta estrutura:
          [
            {
              "id": "step-1",
              "title": "T√≠tulo do Step",
              "description": "Descri√ß√£o",
              "type": "question | offer | result",
              "components": [
                {
                  "type": "text | image | button | form",
                  "content": "conte√∫do do componente",
                  "properties": {}
                }
              ]
            }
          ]`
                },
                {
                    role: 'user',
                    content: prompt
                }
            ],
            temperature: 0.7,
            maxTokens: 3000
        };

        const response = await this.generateContent(request);

        try {
            return JSON.parse(response.content);
        } catch (error) {
            console.error('‚ùå Erro ao parsear JSON da IA:', error);
            throw new Error('IA retornou formato inv√°lido');
        }
    }

    /**
     * üìù Melhorar texto usando IA
     */
    async improveText(text: string, context: string = ''): Promise<string> {
        const request: AIRequest = {
            messages: [
                {
                    role: 'system',
                    content: `Voc√™ √© um copywriter especialista em convers√£o.
          Melhore o texto fornecido tornando-o mais persuasivo e envolvente.
          ${context ? `Contexto: ${context}` : ''}
          Retorne apenas o texto melhorado, sem explica√ß√µes.`
                },
                {
                    role: 'user',
                    content: text
                }
            ],
            temperature: 0.6,
            maxTokens: 1000
        };

        const response = await this.generateContent(request);
        return response.content.trim();
    }

    /**
     * üé® Gerar configura√ß√µes de design usando IA
     */
    async generateDesignConfig(theme: string, brand: string = ''): Promise<any> {
        const request: AIRequest = {
            messages: [
                {
                    role: 'system',
                    content: `Voc√™ √© um designer especialista em UI/UX.
          Gere uma configura√ß√£o de design baseada no tema fornecido.
          ${brand ? `Marca/Empresa: ${brand}` : ''}
          Retorne apenas JSON v√°lido com esta estrutura:
          {
            "primaryColor": "#hexcolor",
            "secondaryColor": "#hexcolor",
            "accentColor": "#hexcolor",
            "backgroundColor": "gradient ou cor s√≥lida",
            "fontFamily": "nome da fonte",
            "button": {
              "background": "cor ou gradient",
              "textColor": "#hexcolor",
              "borderRadius": "px",
              "shadow": "shadow css"
            }
          }`
                },
                {
                    role: 'user',
                    content: `Tema: ${theme}`
                }
            ],
            temperature: 0.5,
            maxTokens: 1000
        };

        const response = await this.generateContent(request);

        try {
            return JSON.parse(response.content);
        } catch (error) {
            console.error('‚ùå Erro ao parsear JSON da IA:', error);
            throw new Error('IA retornou formato inv√°lido');
        }
    }
}

// üîß Factory para criar inst√¢ncia configurada
export function createGitHubModelsAI(token?: string): GitHubModelsAI {
    const aiToken = token || import.meta.env.VITE_GITHUB_MODELS_TOKEN || '';

    if (!aiToken) {
        throw new Error('‚ùå GitHub Models token n√£o configurado. Configure VITE_GITHUB_MODELS_TOKEN no .env');
    }

    return new GitHubModelsAI({
        token: aiToken,
        model: 'gpt-4o-mini', // Modelo r√°pido e gratuito
        maxTokens: 2000,
        temperature: 0.7,
    });
}

export default GitHubModelsAI;